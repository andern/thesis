Consider the quadratic program $\mathcal{Q}$:
\[
\begin{array}{lcrcrcl}
\textrm{minimize}           & &   (x-1)^2 &+&   (y-1)^2 & - &  2 \\
\textrm{subject to}         & &         x &+&         y &\leq& 3 \\
                            & &         x &-&         y &\leq& 1 \\
                            & &         x &+&       3 y &\leq& 4 \\
                 \multicolumn{5}{r}{x,y}                &\geq& 0,
\end{array}
\]
and let $x^*$ denote one of its optimal points and let $f$ denote its objective
function.

We solve $\mathcal{Q}$ using the method described in the previous
section. While solving it, we look at a visual representation of the
method, along with the linear program in standard form.

Let $x_0 = (0,0)$ such that $T_0 = -2x - 2y$. The linear program
$\mathcal{L}_0$ in standard form reads:
\[
\begin{array}{lcrcrl}
    \textrm{minimize}   &-& 2 x &-& 2 y \\
    \textrm{subject to} & &   x &+&   y & \leq 3 \\
                        & &   x &-&   y & \leq 1 \\
                        &-&   x &+& 3 y & \leq 4 \\
     \multicolumn{5}{r}{x,y}            & \geq 0.
\end{array}
\]
Figure \ref{fig:lp1} shows a visual representation of $\mathcal{L}_0$ and
$\mathcal{Q}$.
The figure depicts an initial state
of the method, where you can see the initial guess $x_0$. It also
depicts the unconstrained minimum $x^u$, which in this case
happens to be the same as $x^*$, the optimal solution to
$\mathcal{Q}$. An optimal solution $\hat{x}_0$ to $\mathcal{L}_0$
is also depicted. The striped circles are the contours of $f$.
Note that the feasible region of $\mathcal{L}_k$ and
$\mathcal{Q}$ are equal for all $k$, as the constraints never change during the
iterations of the algorithm. Also note that $\mathcal{L}_0$ has multiple
optimal solutions, so $\hat{x}_0$ may differ between implementations.

\begin{figure}[ht!]
    \centering
    \input{include/slpfirst}
    \caption{A visual representation of $\mathcal{L}_0$ and $\mathcal{Q}$.}
    \label{fig:lp1}
\end{figure}

We perform a line search between the points $x_0$ and $\hat{x}_0$.
We find a function $m$ of $\alpha$ that range all points collinear with $x_0$
and $\hat{x}_0$:
\[
f((1-\alpha) x_0 + \alpha \hat{x}_0) = 5\alpha^2 - 6\alpha = m(\alpha).
\]
To find the minimum of this one-dimensional convex parabola we set
$m^\prime(\alpha) = 0$ and solve for $\alpha$ to achieve $\alpha = 0.6$
(see Figure \ref{fig:steplength}).
\begin{figure}[ht!]
    \centering
    \input{include/slpsecond}
    \caption{Line search between $x_0$ and $\hat{x}_0$. The optimal point
             between $x_0$ and $\hat{x}_0$ becomes $x_1$, our starting point
             for the next iteration.}
    \label{fig:steplength}
\end{figure}
Note that $\alpha \in [0, 1]$ because the unconstrained minimum of $f$ is
inside the feasible region.
Now let
\[
x_1Â = 0.4x_0 + 0.6\hat{x}_0 = (1.2, 0.6),
\]
and let us apply (\ref{eq:texp}) to calculate a Taylor series
expansion at $x_1$:
\[
    T_1 = 0.4x - 0.8y - 1.8.
\]
\begin{comment}
\[
\begin{array}{rcl}
T_1 &=& -\left[
         \begin{array}{cc}
            1.2 & 0.6
         \end{array}
         \right]

         \left[
         \begin{array}{cc}
            1 & 0 \\ 0 & 1
         \end{array}
         \right]
         
         \left[
         \begin{array}{c}
             1.2 \\ [5pt] 0.6
         \end{array}
         \right]
         + 2
         \left[
         \begin{array}{cc}
            1.2 & 0.6
         \end{array}
         \right]
         
         \left[
         \begin{array}{cc}
            1 & 0 \\ 0 & 1
         \end{array}
         \right]
         
         \left[
         \begin{array}{c}
            x \\ y
         \end{array}
         \right] \\
         
     & & +
         \left[
         \begin{array}{cc}
            -2 & -2
         \end{array}
         \right]
         
         \left[
         \begin{array}{c}
            x \\ y
         \end{array}
         \right] \\ [15pt]
    &=& \displaystyle 0.4x - 0.8y - 1.8
\end{array}
\]
\end{comment}

Now we minimize $T_1$ subject to the original constraints and call the LP
problem for $\mathcal{L}_1$.
The linear program of $\mathcal{L}_1$ in standard form reads:
\[
\begin{array}{lcrcrl}
    \textrm{minimize}   & & 0.4 x &-& 0.8 y \\
    \textrm{subject to} & &     x &+&     y & \leq 3 \\
                        & &     x &-&     y & \leq 1 \\
                        &-&     x &+&   3 y & \leq 4 \\
     \multicolumn{5}{r}{x,y}                & \geq 0.
\end{array}
\]
We solve $\mathcal{L}_1$ and achieve an optimal solution $\hat{x}_1$
(see Figure \ref{fig:lp2}).

\begin{figure}[ht!]
\centering
\input{include/slpthird}
\caption{A visual representation of $\mathcal{L}_1$ and $\mathcal{Q}$.
         Note that $x_1$ now has a red line through it. This is
         the objective function of $\mathcal{L}_1$.}
\label{fig:lp2}
\end{figure}

Again we perform a line search, but this time between the points $x_1$ and
%$\hat{x}_1$ and find that $\alpha = 0.73$, $x_2 = (0.88, 0.8)$ and
$\hat{x}_1$ and find that $\alpha = 0.27$, $x_2 = (0.88, 0.8)$ and
$T_2 = -0.25x -0.4y -0.4$.
Minimize $T_2$ subject to the original constraints and call the LP problem for
$\mathcal{L}_2$.
The linear program of $\mathcal{L}_2$ in standard form reads:
\[
\begin{array}{lcrcrl}
    \textrm{minimize}   &-& 0.25 x &-& 0.4 y & \\
    \textrm{subject to} & &      x &+&     y & \leq 3 \\
                        & &      x &-&     y & \leq 1 \\
                        &-&      x &+&   3 y & \leq 4 \\
      \multicolumn{5}{r}{x,y}                & \geq 0.
\end{array}
\]
We solve $\mathcal{L}_2$ and achieve an optimal solution $\hat{x}_2
= (1.25, 1.75)$ (see Figure \ref{fig:lp3}).
\begin{figure}[ht!]
\centering
\input{include/slpfourth}
\caption{A visual representation of $\mathcal{L}_2$ and $\mathcal{Q}$.}
\label{fig:lp3}
\end{figure}

Performing line search between $x_2$ and $\hat{x}_2$ we get that
$\alpha = 0.77$ and that $x_3 = (0.96, 1.02)$.
Assume $\epsilon = 0.5$, which imples that
$\frac{f(x_2) - f(x_3)}{|f(x_2)|} \leq \epsilon$, so we stop and return $x_3$
as our solution.

\begin{figure}[ht!]
\centering
\input{include/slppath}
\caption{Illustration of the path taken by the algorithm.}
\label{fig:slppath}
\end{figure}

Figure \ref{fig:slppath} shows the iterates of the algorithm up until
the termination condition is met.
