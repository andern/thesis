One interesting property shared by several practical applications is that the
coefficients in the linear term of the objective function is of a much higher
magnitude than those in the quadratic term.
From this, one might think that the quadratic term is close to negligible.
By removing the quadratic term from the objective function, leaving only the
linear term, the QP problem transforms into an LP problem.
Linearizing the objective function using a first-order Taylor series expansion
at 0 would give us the same result.

Consider a first-order (linear) Taylor series expansion of a
general function $f$ at a point $a$:
\[
T_a(x) = f(a) + \nabla f(a)^T(x-a),
\]
where $\nabla f(a)$ is the gradient of $f$ evaluated at $a$~\cite{apostol}.
A linear approximation of function (\ref{eq:obj}) becomes
\[
T_a(x) = a^THa + b^Ta + (2a^TH + b^T)(x - a).
\]
There are a number of terms that cancel each other out. Cancelling them,
the approximation becomes
\begin{equation}
\label{eq:texp}
T_a(x) = - a^THa + 2a^THx + b^Tx.
\end{equation}
Now, if $a = 0$ we are left with
\[
T_0(x) = b^Tx.
\]
Generally, for any QP problem $\mathcal{Q}$ as formulated in
(\ref{eq:thesisqp}), there exists an associated LP problem $\mathcal{L}$
formulated as:
\[
\min_{x \in \mathbb{R}^n} g(x)
\quad \textrm{subject to}
~
Ax = 0,
~
l \le x \le u,
\]
where $g(x) = T_0(x)$.
%We introduce a new function $g(x) = T_0(x)$, and we formulate a new LP problem
%that we call $\mathcal{L}$:
%\[
%\min_{x \in \mathbb{R}^n} g(x)
%\quad \textrm{subject to}
%~
%Ax = 0,
%~
%l \le x \le u.
%\]
We let $\hat{x}$ denote some optimal solution of $\mathcal{L}$, and let $x^*$
denote some optimal solution of $\mathcal{Q}$.

We can get an idea about the quality of
$\mathcal{L}$'s solution with respect to $\mathcal{Q}$ by substituting
$\hat{x}$ into (\ref{eq:obj}) and seeing how much the numerical value
deviates from the optimal objective value of $\mathcal{Q}$.
For any QP problem $\mathcal{Q}$ and its associated LP problem $\mathcal{L}$,
we denote this deviation by
\[
\Delta = \left|\frac{f(\hat{x}) - f(x^*)}{f(x^*)}\right|.
\]

Figure \ref{fig:sparsobj} shows how $\Delta$ changes in respect to the sparsity
in the objective function, both in the linear and the quadratic terms, on
randomly generated instances with $200$ edges and $70$ nodes\footnote{
We discuss the
characteristics of the randomly generated instances in the beginning of Chapter
\ref{ch:experiments}.}.
The percentage of zeroes in the diagonal elements of $H$ is represented
by $x$, while the percentage of zeroes in $b$ is represented by $y$.
Each data point represents the average $100\Delta$ over
$100$ randomly generated networks. 

\begin{figure}[ht!]
\begin{center}
%    Hidden. TODO: Show
    \input{include/delta3d}
\end{center}
\caption{Deviation as a function of density in the objective function.}
\label{fig:sparsobj}
\end{figure}

We can see that the density in the diagonal elements of $H$ has
close to zero influence on the deviation.
While we see that the density of $b$ certainly has more influence,
the deviation actually never change more than around $4\%$.

These results give us incentive to look into solution methods that
are based upon linear programming.
