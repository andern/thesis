While implementing an algorithm, it makes sense to start to implement
sub-functions that are independent of other functions.

Implementing the Taylor series expansion is pretty straight forward, as
we have a closed-form definition for the specific objective function that
we described in Chapter \ref{ch:qp}. The function prototype for the
Taylor series expansion reads:
\begin{verbatim}
void taylor(double* T, const double* x,
            const ClpModel& m);
\end{verbatim}
where \texttt{ClpModel} gives us access to both the linear and quadratic
objective term. The result of the Taylor-series expansion, i.e. the 
coefficients of the new objective function, is put in \texttt{T}.

The next step in the Slp algorithm is to solve the new linear program with the
new objective function \texttt{T}. This is simply done with a call to Clp.

After solving the linear program, the next step is to find the one-dimensional
minimizer between our current point and the solution to the linear program we
just solved.
The step length $\alpha_k$ tells us where this minimizer lies between the two
points.
To implement this line search, we find a closed-form definition of the step
length $\alpha_k$ by letting $m(\alpha) = f((1-\alpha) x_k + \alpha \hat{x}_k)$
and setting $m^\prime(\alpha) = 0$ and solving for $\alpha$:
\[
\alpha_k = \frac{
                2x_k^T H x_k
                - 2\hat{x}_k^T H x_k
                + b^T x_k - b^T \hat{x}_k
                }{
                  2\hat{x}_k^T H \hat{x}_k
                - 4\hat{x}_k^T H x_k
                + 2x_k^T H x_k
                }
\]
The function prototype for the line search function reads:
\begin{verbatim}
double lineSearch(const double* p,
                  const double* q,
                  const ClpModel& m);
\end{verbatim}
where \texttt{ClpModel} gives us access to both the linear and quadratic
objective term.

The termination condition of Slp depends on the objective value of the two
previous iterations. So, in order to terminate, we need a function that can
compute the objective value. The function prototype for this function reads:
\begin{verbatim}
double objVal(const double* p,
              const ClpModel& m);
\end{verbatim}
where \texttt{ClpModel} again gives us access to both the linear and quadratic
objective term.

These three functions all have a linear running time in the order of the number
of columns in the problem. The running times are dependent on the $b$ vector,
which is not stored sparsely.

When it comes to decisions to be made around data types and storage formats,
there is not really too much to be said, because the choices are quite limited.
It is pretty much dependant on the linear programming solver that is used.
As mentioned earlier, Clp stores matrices in a sparse format, and all the sub
functions only operate on non-zero elements in those matrices, so there is not
really room for much overhead.

An implementation of Algorithm \ref{alg:iter} in \texttt{C++} looks like this:
\begin{verbatim}
ClpModel lin = m; // A linear copy of m
do {
  taylor(T, x, m);
  // Set new linear objective
  lin.chgObjCoefficients(T);
  // Run the primal simplex method
  lin.primal();

  const double* xhat = 
          lin.primalColumnSolution();
  double alpha = lineSearch(x, xhat, m);
  if (alpha > 1) alpha = 1;

  double oldval = objVal(x, m);
  for (int i = 0; i < numCols; i++) {
      x[i] = (1-alpha) * xhat[i];
  }
  double val = objVal(x, m);

  stop = (oldval - val);
  stop /= fabs(oldval);
} while (stop > tolerance);
\end{verbatim}
